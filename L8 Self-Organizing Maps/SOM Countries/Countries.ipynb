{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "exampleSOM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "de5vaR9hwAhb"
   },
   "source": [
    "## Original code from: Self-Organizing Maps by Paras Chopra and Jorge Trigueros\n",
    "from random import *\n",
    "from math import *\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, feature_size=10, prediction_size=10, x=0, y=0):\n",
    "        self.feature_size = feature_size\n",
    "        self.prediction_size = prediction_size\n",
    "        self.feature_vector = [0.0] * feature_size              # Feature Vector\n",
    "        self.prediction_vector = [0.0] * prediction_size        # Prediction Vector\n",
    "        self.x = x                                              # X location\n",
    "        self.y = y                                              # Y location\n",
    "        self.feature_vector = [random() for _ in range(feature_size)]\n",
    "        self.prediction_vector = [random() for _ in range(prediction_size)]\n",
    "\n",
    "\n",
    "class SOM:\n",
    "    # Let distance=False if you want to auto-calculate the distance\n",
    "    def __init__(self, height=10, width=10, feature_size=10, prediction_size=10, distance=False, learning_rate=0.005):\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.distance = distance if distance else (height+width)/2\n",
    "        self.total = height * width\n",
    "        self.learning_rate = learning_rate\n",
    "        self.nodes = [0] * self.total\n",
    "        self.feature_size = feature_size\n",
    "        self.prediction_size = prediction_size\n",
    "        \n",
    "        for i in range(self.height):\n",
    "            for j in range(self.width):\n",
    "                self.nodes[i*self.width + j] = Node(feature_size, prediction_size, i, j)\n",
    "\n",
    "\n",
    "    # Train_vector format: [ [feature_vector[0], prediction_vector[0]],\n",
    "    #                        [feature_vector[1], prediction_vector[1]], so on..\n",
    "    def train(self, train_vector, iterations=1000):\n",
    "        time_constant = iterations / log(self.distance)\n",
    "\n",
    "        for i in range(1,iterations+1):\n",
    "            if i % 10 == 0: print(i, end=', ')\n",
    "\n",
    "            distance_decaying = self.distance * exp(-1.0 * i/time_constant)\n",
    "            learning_rate_decaying = self.learning_rate * exp(-1.0*i/time_constant)\n",
    "            \n",
    "            for j in range(len(train_vector)):\n",
    "                input_feature = train_vector[j][0]\n",
    "                input_prediction = train_vector[j][1]\n",
    "                best = self.best_match(input_feature)\n",
    "                \n",
    "                stack = []\n",
    "                for k in range(self.total):\n",
    "                    distance = SOM.distance(self.nodes[best], self.nodes[k])\n",
    "\n",
    "                    if distance < distance_decaying:\n",
    "                        temporal_feature = [0.0] * self.feature_size\n",
    "                        temporal_prediction=[0.0]*self.prediction_size\n",
    "                        influence = exp((-1.0*(distance**2)) / (2*distance_decaying*i))\n",
    "\n",
    "                        for l in range(self.feature_size):      # Learning\n",
    "                            temporal_feature[l] = self.nodes[k].feature_vector[l] + influence * learning_rate_decaying \\\n",
    "                                                  * (input_feature[l] - self.nodes[k].feature_vector[l])\n",
    "\n",
    "                        for l in range(self.prediction_size):   # Learning\n",
    "                            temporal_prediction[l] = self.nodes[k].prediction_vector[l] + influence * learning_rate_decaying \\\n",
    "                                                     * (input_prediction[l] - self.nodes[k].prediction_vector[l])\n",
    "\n",
    "                        # Push the unit onto stack to update in next interval\n",
    "                        stack[0:0] = [[[k], temporal_feature, temporal_prediction]]\n",
    "\n",
    "                for l in range(len(stack)):\n",
    "                    self.nodes[stack[l][0][0]].feature_vector[:] = stack[l][1][:]\n",
    "                    self.nodes[stack[l][0][0]].prediction_vector[:] = stack[l][2][:]\n",
    "\n",
    "\n",
    "    # Returns prediction vector\n",
    "    def predict(self, input_vector, return_coors=False):\n",
    "        best = self.best_match(input_vector)\n",
    "\n",
    "        if return_coors:\n",
    "          return self.nodes[best].prediction_vector, self.nodes[best].x, self.nodes[best].y\n",
    "\n",
    "        return self.nodes[best].prediction_vector\n",
    "    \n",
    "    \n",
    "    # Returns best matching unit's index\n",
    "    def best_match(self, target_feature):\n",
    "        minimum = sqrt(self.feature_size)               # Minimum distance\n",
    "        minimum_index = 1                               # Minimum distance unit\n",
    "        \n",
    "        for i in range(self.total):\n",
    "            temp = self.feature_distance(self.nodes[i].feature_vector, target_feature)\n",
    "            if temp < minimum:\n",
    "                minimum = temp\n",
    "                minimum_index = i\n",
    "        \n",
    "        return minimum_index\n",
    "\n",
    "    def feature_distance(self, feature1, feature2):\n",
    "        temp=0.0\n",
    "        \n",
    "        for j in range(self.feature_size):\n",
    "            temp += (feature1[j]-feature2[j])**2\n",
    "\n",
    "        temp = sqrt(temp)\n",
    "        return temp\n",
    "\n",
    "    @staticmethod\n",
    "    def distance(n1: Node, n2: Node):\n",
    "        return sqrt((n1.x - n2.x)**2 + (n1.y - n2.y)**2)\n"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preparación de los datos y la red"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2y7Emb3sxie_",
    "outputId": "b885bc39-53ff-49fb-ebe0-a8d85c9ebadb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "HEIGHT = 30\n",
    "WIDTH = 30\n",
    "FEATURE_SIZE = 10\n",
    "PREDICTION_SIZE = 1\n",
    "\n",
    "# for year in ['2013', '2014', '2015']:\n",
    "data = pd.read_csv('2013' + '.csv')\n",
    "countries = data.columns[2:]\n",
    "data = data[countries]\n",
    "data = (data - data.mean()) / data.std()\n",
    "training_set = []\n",
    "kohonen_network = SOM(HEIGHT, WIDTH, FEATURE_SIZE, PREDICTION_SIZE, distance=False, learning_rate=0.05)\n",
    "\n",
    "for index in range(len(countries)):\n",
    "    training_set.append([data[countries[index]].to_numpy(), [index]])"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Entrenamiento"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, 280, 290, 300, 310, 320, 330, 340, 350, 360, 370, 380, 390, 400, 410, 420, 430, 440, 450, 460, 470, 480, 490, 500, 510, 520, 530, 540, 550, 560, 570, 580, 590, 600, 610, 620, 630, 640, 650, 660, 670, 680, 690, 700, 710, 720, 730, 740, 750, 760, 770, 780, 790, 800, 810, 820, 830, 840, 850, 860, 870, 880, 890, 900, 910, 920, 930, 940, 950, 960, 970, 980, 990, 1000, 1010, 1020, 1030, 1040, 1050, 1060, 1070, 1080, 1090, 1100, 1110, 1120, 1130, 1140, 1150, 1160, 1170, 1180, 1190, 1200, 1210, 1220, 1230, 1240, 1250, 1260, 1270, 1280, 1290, 1300, 1310, 1320, 1330, 1340, 1350, 1360, 1370, 1380, 1390, 1400, 1410, 1420, 1430, 1440, 1450, 1460, 1470, 1480, 1490, 1500, 1510, 1520, 1530, 1540, 1550, 1560, 1570, 1580, 1590, 1600, 1610, 1620, 1630, 1640, 1650, 1660, 1670, 1680, 1690, 1700, 1710, 1720, 1730, 1740, 1750, 1760, 1770, 1780, 1790, 1800, 1810, 1820, 1830, 1840, 1850, 1860, 1870, 1880, 1890, 1900, 1910, 1920, 1930, 1940, 1950, 1960, 1970, 1980, 1990, 2000, "
     ]
    }
   ],
   "source": [
    "ITERATIONS = 2000\n",
    "kohonen_network.train(training_set, ITERATIONS)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pruebas"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: United States [USA]      Output: United States [USA]  Coors: (3, 7)\n",
      "Input: United Kingdom [GBR]      Output: Vietnam [VNM]  Coors: (1, 10)\n",
      "Input: Vietnam [VNM]      Output: Switzerland [CHE]  Coors: (9, 1)\n",
      "Input: Switzerland [CHE]      Output: Switzerland [CHE]  Coors: (0, 14)\n",
      "Input: Spain [ESP]      Output: Spain [ESP]  Coors: (11, 14)\n",
      "Input: Qatar [QAT]      Output: Peru [PER]  Coors: (0, 1)\n",
      "Input: Portugal [PRT]      Output: Portugal [PRT]  Coors: (12, 10)\n",
      "Input: Peru [PER]      Output: Panama [PAN]  Coors: (5, 2)\n",
      "Input: Panama [PAN]      Output: Norway [NOR]  Coors: (12, 6)\n",
      "Input: Norway [NOR]      Output: Nicaragua [NIC]  Coors: (1, 0)\n",
      "Input: New Zealand [NZL]      Output: Netherlands [NLD]  Coors: (0, 8)\n",
      "Input: Nicaragua [NIC]      Output: Netherlands [NLD]  Coors: (14, 2)\n",
      "Input: Netherlands [NLD]      Output: Mexico [MEX]  Coors: (4, 4)\n",
      "Input: Mexico [MEX]      Output: Netherlands [NLD]  Coors: (7, 0)\n",
      "Input: Luxembourg [LUX]      Output: Luxembourg [LUX]  Coors: (14, 14)\n",
      "Input: Japan [JPN]      Output: Japan [JPN]  Coors: (8, 14)\n",
      "Input: Italy [ITA]      Output: Italy [ITA]  Coors: (10, 11)\n",
      "Input: Israel [ISR]      Output: Jamaica [JAM]  Coors: (6, 11)\n",
      "Input: Jamaica [JAM]      Output: Israel [ISR]  Coors: (11, 8)\n",
      "Input: India [IND]      Output: India [IND]  Coors: (8, 9)\n",
      "Input: Iceland [ISL]      Output: Jamaica [JAM]  Coors: (1, 7)\n",
      "Input: Greece [GRC]      Output: Greece [GRC]  Coors: (14, 11)\n",
      "Input: Finland [FIN]      Output: Finland [FIN]  Coors: (4, 14)\n",
      "Input: France [FRA]      Output: France [FRA]  Coors: (2, 13)\n",
      "Input: Germany [DEU]      Output: France [FRA]  Coors: (3, 11)\n",
      "Input: Egypt, Arab Rep. [EGY]      Output: Germany [DEU]  Coors: (7, 3)\n",
      "Input: Ethiopia [ETH]      Output: Ethiopia [ETH]  Coors: (11, 0)\n",
      "Input: Ecuador [ECU]      Output: Ecuador [ECU]  Coors: (4, 0)\n",
      "Input: Cuba [CUB]      Output: Ecuador [ECU]  Coors: (14, 8)\n",
      "Input: Costa Rica [CRI]      Output: Costa Rica [CRI]  Coors: (13, 4)\n",
      "Input: Chile [CHL]      Output: Chile [CHL]  Coors: (10, 6)\n",
      "Input: Canada [CAN]      Output: Canada [CAN]  Coors: (1, 5)\n",
      "Input: Colombia [COL]      Output: Canada [CAN]  Coors: (2, 1)\n",
      "Input: China [CHN]      Output: China [CHN]  Coors: (5, 9)\n",
      "Input: Brazil [BRA]      Output: Brazil [BRA]  Coors: (11, 3)\n",
      "Input: Australia [AUS]      Output: Australia [AUS]  Coors: (0, 3)\n",
      "Input: Argentina [ARG]      Output: Argentina [ARG]  Coors: (9, 4)\n",
      "Input: Afghanistan [AFG]      Output: Argentina [ARG]  Coors: (14, 0)\n",
      "Input: Ireland [IRL]      Output: Afghanistan [AFG]  Coors: (6, 13)\n",
      "Input: Poland [POL]      Output: Poland [POL]  Coors: (6, 5)\n",
      "Input: Romania [ROU]      Output: Romania [ROU]  Coors: (7, 7)\n",
      "Número de aciertos: 22 de 41\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "results = pd.DataFrame()\n",
    "\n",
    "for index in range(len(countries)):\n",
    "    prediction = kohonen_network.predict(data[countries[index]].to_numpy(), True)\n",
    "    print(f'Input: {countries[index]}      Output: {countries[round(prediction[0][0])]}  Coors: {prediction[1:]}')\n",
    "    if countries[index] == countries[round(prediction[0][0])]: count = count + 1\n",
    "\n",
    "print(f'Número de aciertos: {count} de {len(countries)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Impresión del mapa"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "grid = []\n",
    "\n",
    "for col in range(HEIGHT):\n",
    "    row = [round(kohonen_network.nodes[i * WIDTH + col].prediction_vector[0]) for i in range(WIDTH)]\n",
    "    grid.append(row)\n",
    "\n",
    "grid_frame = pd.DataFrame(grid)\n",
    "grid_frame.to_csv('map.csv', header=False, index=False)\n",
    "print(grid_frame)\n",
    "\n",
    "for i in range(len(countries)):\n",
    "    country = countries[i]\n",
    "    grid_frame = grid_frame.replace(i, country[-5:])\n",
    "\n",
    "grid_frame.to_csv('map_names.csv', header=False, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "count = []\n",
    "three_letters_countries = []\n",
    "\n",
    "for country in countries:\n",
    "    three_letters_countries.append(country[-5:])\n",
    "    count.append(grid_frame[grid_frame == country[-5:]].count().sum())\n",
    "\n",
    "countries_frame = pd.DataFrame(three_letters_countries, count)\n",
    "countries_frame.to_csv('countries.csv', header=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}